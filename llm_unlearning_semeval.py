# -*- coding: utf-8 -*-
"""llm_unlearning_semeval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13457HjAdUFMsa5S4XvxTHH7r_XgLyKKS

Prereqs
"""

!pip install --upgrade transformers huggingface_hub ai2-olmo datasets rouge_score

# comp token
hf_token = ""

# Grab the competition model
from huggingface_hub import snapshot_download

# snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-finetuned-semeval25-unlearning', token=hf_token, local_dir='semeval25-unlearning-model')
snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', token=hf_token, local_dir='semeval25-unlearning-1B-model')

## Fetch and load dataset:
snapshot_download(repo_id='llmunlearningsemeval2025organization/semeval25-unlearning-dataset-public', token=hf_token, local_dir='semeval25-unlearning-data', repo_type="dataset")

import pandas as pd
retain_train_df = pd.read_parquet('semeval25-unlearning-data/data/retain_train-00000-of-00001.parquet', engine='pyarrow') # Retain split: train set
retain_validation_df = pd.read_parquet('semeval25-unlearning-data/data/retain_validation-00000-of-00001.parquet', engine='pyarrow') # Retain split: validation set

forget_train_df = pd.read_parquet('semeval25-unlearning-data/data/forget_train-00000-of-00001.parquet', engine='pyarrow') # Forget split: train set
forget_validation_df = pd.read_parquet('semeval25-unlearning-data/data/forget_validation-00000-of-00001.parquet', engine='pyarrow') # Forget split: validation set



"""Poke the sent datasets"""

retain_train_df.head()

# forget_train_df.head(30)

# print(forget_train_df.head(30)["input"].values)
# print(forget_train_df.head(30)["output"].values)

"""Get the fine-tuned competition model"""

from transformers import AutoTokenizer, AutoModelForCausalLM
sem_eval_model = AutoModelForCausalLM.from_pretrained("semeval25-unlearning-1B-model")
# tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-7B-0724-Instruct-hf")
tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-1B-0724-hf")

sem_eval_model.to("cuda")

message = ["Language modelling is "]

def inference(message: str) -> str:
  inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)
  inputs = inputs.to("cuda")
  response = sem_eval_model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
  inf_result = tokenizer.batch_decode(response, skip_special_tokens=True)
  return inf_result[0]

result = inference(message)

messages_to_probe_with = retain_train_df["input"].values[0:5]
outputs_to_compare = retain_train_df["output"].values[0:5]

from rouge_score import rouge_scorer
import statistics
from statistics import mean

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

results = []
for i, message in enumerate(messages_to_probe_with):
  # run inference on the retain inputs
  result = inference(message)
  # remove the retained input from the result
  timmed_result = result.replace(message, "")
  print(f"Original input \"{message}\", \nProbe: \"{timmed_result}\", \nOriginal output: {outputs_to_compare[i]}\n\n")
  score = scorer.score(timmed_result, outputs_to_compare[i])
  results.append(score["rougeL"].fmeasure)


print(f"All results: {results}")
print(f"Average results: {mean(results)}")

